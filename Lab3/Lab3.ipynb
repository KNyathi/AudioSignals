{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8038cc3",
   "metadata": {},
   "source": [
    "## Лабораторная Работа 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb13fb",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31aa97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-zz3jzjdf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-zz3jzjdf\n",
      "  Resolved https://github.com/huggingface/transformers to commit 9aab965b1e61d92d402809bd467c317ec464e560\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==5.0.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
      "Requirement already satisfied: requests in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub==1.0.0.rc6 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (1.0.0rc6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
      "Requirement already satisfied: typer-slim in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (2025.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->transformers==5.0.0.dev0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==5.0.0.dev0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==5.0.0.dev0) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->transformers==5.0.0.dev0) (2.10)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/nyathi/.local/lib/python3.10/site-packages (from typer-slim->transformers==5.0.0.dev0) (8.1.8)\n",
      "Requirement already satisfied: anyio in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/nyathi/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077d1cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faster-whisper in /home/nyathi/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: vosk in /home/nyathi/.local/lib/python3.10/site-packages (0.3.45)\n",
      "Requirement already satisfied: jiwer in /home/nyathi/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: librosa in /home/nyathi/.local/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /home/nyathi/.local/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: pydub in /home/nyathi/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: torch in /home/nyathi/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in /home/nyathi/.local/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: torchaudio in /home/nyathi/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: gtts in /home/nyathi/.local/lib/python3.10/site-packages (2.5.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (1.0.0rc6)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (4.6.0)\n",
      "Requirement already satisfied: tqdm in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (4.67.1)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (1.23.1)\n",
      "Requirement already satisfied: av>=11 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (16.0.1)\n",
      "Requirement already satisfied: tokenizers<1,>=0.13 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (0.22.1)\n",
      "Requirement already satisfied: srt in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: requests in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (2.32.5)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (2.0.0)\n",
      "Requirement already satisfied: websockets in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (15.0.1)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/nyathi/.local/lib/python3.10/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/nyathi/.local/lib/python3.10/site-packages (from jiwer) (3.14.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.24.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: filelock in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: fsspec in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: jinja2 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: networkx in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: packaging in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: xxhash in /home/nyathi/.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /home/nyathi/.local/lib/python3.10/site-packages (from cffi>=1.0->vosk) (2.23)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.9.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/nyathi/.local/lib/python3.10/site-packages (from fsspec->torch) (3.13.1)\n",
      "Requirement already satisfied: anyio in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2.10)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/nyathi/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.10)\n",
      "Requirement already satisfied: typer-slim in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (0.20.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /home/nyathi/.local/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
      "Requirement already satisfied: flatbuffers in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.25.8)\n",
      "Requirement already satisfied: coloredlogs in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/nyathi/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->vosk) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->vosk) (1.26.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyathi/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nyathi/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nyathi/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (6.7.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (0.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (25.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nyathi/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/nyathi/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faster-whisper vosk jiwer librosa soundfile pydub torch datasets torchaudio gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db29977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faster_whisper import WhisperModel\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import jiwer\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7296",
   "metadata": {},
   "source": [
    "### Download Vosk and Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53732d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete\n",
      "Extraction complete\n",
      "Cleanup complete\n",
      "Model contents: ['README', 'ivector', 'am', 'graph', 'conf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-ru-0.22/graph/HCLr.fst vosk-model-small-ru-0.22/graph/Gr.fst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOSK MODEL IS WORKING!\n",
      "Model path: /home/nyathi/AudioSignals/Lab3/vosk-model-small-ru-0.22\n",
      "\n",
      "READY! You can now run your main ASR comparison.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n"
     ]
    }
   ],
   "source": [
    "def download_and_setup_vosk():\n",
    "    model_url = \"https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip\"\n",
    "    model_zip = \"vosk-model-small-ru-0.22.zip\"\n",
    "    model_dir = \"vosk-model-small-ru-0.22\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Download the model\n",
    "        urllib.request.urlretrieve(model_url, model_zip)\n",
    "        print(\"Download complete\")\n",
    "        \n",
    "        # Extract the model\n",
    "        with zipfile.ZipFile(model_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        print(\"Extraction complete\")\n",
    "        \n",
    "        # Clean up zip file\n",
    "        os.remove(model_zip)\n",
    "        print(\"Cleanup complete\")\n",
    "        \n",
    "        # Verify the model structure\n",
    "        if os.path.exists(model_dir):\n",
    "            contents = os.listdir(model_dir)\n",
    "            print(f\"Model contents: {contents}\")\n",
    "            \n",
    "            # Test if model works\n",
    "            from vosk import Model\n",
    "            try:\n",
    "                test_model = Model(model_dir)\n",
    "                print(\"VOSK MODEL IS WORKING!\")\n",
    "                print(f\"Model path: {os.path.abspath(model_dir)}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Model test failed: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Model directory not created\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = download_and_setup_vosk()\n",
    "    if success:\n",
    "        print(\"\\nREADY! You can now run your main ASR comparison.\")\n",
    "    else:\n",
    "        print(\"\\nFailed to set up Vosk model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbd5d7",
   "metadata": {},
   "source": [
    "### Checking GigaAM installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9205c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyathi/GigaAM/gigaam/__init__.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "WARNING:root:fp16 is not supported on CPU. Leaving fp32 weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAMASR(\n",
      "  (preprocessor): FeatureExtractor(\n",
      "    (featurizer): Sequential(\n",
      "      (0): MelSpectrogram(\n",
      "        (spectrogram): Spectrogram()\n",
      "        (mel_scale): MelScale()\n",
      "      )\n",
      "      (1): SpecScaler()\n",
      "    )\n",
      "  )\n",
      "  (encoder): ConformerEncoder(\n",
      "    (pre_encode): StridingSubsampling(\n",
      "      (out): Linear(in_features=12288, out_features=768, bias=True)\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pos_enc): RotaryPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x ConformerLayer(\n",
      "        (norm_feed_forward1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward1): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_conv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): ConformerConvolution(\n",
      "          (pointwise_conv1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
      "          (batch_norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): SiLU()\n",
      "          (pointwise_conv2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (norm_self_att): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): RotaryPositionMultiHeadAttention(\n",
      "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_feed_forward2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward2): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_out): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): CTCHead(\n",
      "    (decoder_layers): Sequential(\n",
      "      (0): Conv1d(768, 34, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import gigaam\n",
    "model = gigaam.load_model(\"ctc\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7766aff",
   "metadata": {},
   "source": [
    "### Create audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c82d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom gtts import gTTS\\nimport os\\n\\ndef create_audio_files():\\n    \"\"\"Create Russian audio files using Google TTS\"\"\"\\n    \\n    # Create directories\\n    os.makedirs(\"raw_audio\", exist_ok=True)\\n    os.makedirs(\"reference_texts\", exist_ok=True)\\n    \\n    # Russian texts for main comparison (10 files)\\n    main_texts = [\\n        # Longer, more complex sentences with varied vocabulary\\n        \"В современном мире искусственный интеллект стремительно развивается и находит применение в различных сферах человеческой деятельности, начиная от медицины и заканчивая финансовыми технологиями.\",\\n        \\n        \"Несмотря на значительные достижения в области машинного обучения, создание по-настоящему разумных систем, способных к абстрактному мышлению и творчеству, остается сложнейшей задачей для исследователей.\",\\n        \\n        \"Русский язык, являясь одним из самых богатых и выразительных языков мира, обладает сложной грамматической структурой и многообразием стилистических средств, что представляет особый интерес для лингвистического анализа.\",\\n        \\n        \"Климатические изменения, наблюдаемые в последние десятилетия, оказывают profound влияние на экосистемы по всей планете, вызывая беспокойство у научного сообщества и требуя незамедлительных мер.\",\\n        \\n        \"Экономическая глобализация, с одной стороны, способствует развитию международной торговли и культурному обмену, а с другой — создает определенные challenges для национальных экономик и социальных структур.\",\\n        \\n        \"Квантовые вычисления, основанные на принципах квантовой механики, promise революционизировать области криптографии, молекулярного моделирования и оптимизации сложных систем в ближайшие годы.\",\\n        \\n        \"Философские концепции, разработанные античными мыслителями, продолжают оказывать значительное влияние на современную этику, политическую теорию и методологию научного познания.\",\\n        \\n        \"Биотехнологический прогресс последних лет позволяет редактировать геномы живых организмов с unprecedented точностью, открывая новые возможности в медицине и сельском хозяйстве.\",\\n        \\n        \"Архитектурные памятники древних цивилизаций, сохранившиеся до наших дней, представляют собой не только историческую ценность, но и источник вдохновения для современных зодчих.\",\\n        \\n        \"Психолингвистические исследования демонстрируют, как языковые структуры влияют на процессы восприятия, категоризации и запоминания информации у носителей разных языков.\"\\n    ]\\n\\n    domain_texts = [\\n        # Technical/scientific texts with complex terminology\\n        \"Трансформерные архитектуры, основанные на механизмах внимания, кардинально изменили подходы к обработке естественного языка, демонстрируя state-of-the-art результаты в задачах машинного перевода и генерации текста.\",\\n        \\n        \"Глубокое обучение с подкреплением, сочетающее в себе нейронные сети и методы динамического программирования, успешно применяется для решения сложных задач управления в robotics и игровых симуляторах.\",\\n        \\n        \"Диффузионные probabilistic модели, генерирующие данные через процесс последовательной денойзинга, показали remarkable результаты в синтезе изображений высокого разрешения и аудиосигналов.\",\\n        \\n        \"Метаобучение или learning-to-learn подходы позволяют моделям быстро адаптироваться к новым задачам на основе limited количества примеров, имитируя способность человека к быстрому обучению.\",\\n        \\n        \"Мультимодальные нейронные сети, способные одновременно обрабатывать и интегрировать информацию из различных источников such as текст, изображения и аудио, открывают новые перспективы для создания более интеллектуальных AI систем.\"\\n    ]\\n    \\n    print(\"Creating main audio files with Google TTS...\")\\n    \\n    # Create main audio files\\n    for i, text in enumerate(main_texts, 1):\\n        # Create audio using Google TTS\\n        tts = gTTS(text=text, lang=\\'ru\\', slow=False)\\n        audio_file = f\"raw_audio/main_audio_{i}.wav\"\\n        tts.save(audio_file)\\n        print(f\"Created: {audio_file}\")\\n        \\n        # Save reference text\\n        with open(f\"reference_texts/audio{i}.txt\", \"w\", encoding=\"utf-8\") as f:\\n            f.write(text)\\n        print(f\" Created: reference_texts/audio{i}.txt\")\\n    \\n    print(\"\\nCreating domain audio files...\")\\n    \\n    # Create domain audio files\\n    for i, text in enumerate(domain_texts, 1):\\n        # Create audio using Google TTS\\n        tts = gTTS(text=text, lang=\\'ru\\', slow=False)\\n        audio_file = f\"raw_audio/domain_audio_{i}.wav\"\\n        tts.save(audio_file)\\n        print(f\"Created: {audio_file}\")\\n        \\n        # Save reference text\\n        with open(f\"reference_texts/domain{i}.txt\", \"w\", encoding=\"utf-8\") as f:\\n            f.write(text)\\n        print(f\"Created: reference_texts/domain{i}.txt\")\\n    \\n    print(f\"\\n Created {len(main_texts)} main audio files and {len(domain_texts)} domain audio files!\")\\n\\nif __name__ == \"__main__\":\\n    create_audio_files()\\n    \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "def create_audio_files():\n",
    "    \"\"\"Create Russian audio files using Google TTS\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"raw_audio\", exist_ok=True)\n",
    "    os.makedirs(\"reference_texts\", exist_ok=True)\n",
    "    \n",
    "    # Russian texts for main comparison (10 files)\n",
    "    main_texts = [\n",
    "        # Longer, more complex sentences with varied vocabulary\n",
    "        \"В современном мире искусственный интеллект стремительно развивается и находит применение в различных сферах человеческой деятельности, начиная от медицины и заканчивая финансовыми технологиями.\",\n",
    "        \n",
    "        \"Несмотря на значительные достижения в области машинного обучения, создание по-настоящему разумных систем, способных к абстрактному мышлению и творчеству, остается сложнейшей задачей для исследователей.\",\n",
    "        \n",
    "        \"Русский язык, являясь одним из самых богатых и выразительных языков мира, обладает сложной грамматической структурой и многообразием стилистических средств, что представляет особый интерес для лингвистического анализа.\",\n",
    "        \n",
    "        \"Климатические изменения, наблюдаемые в последние десятилетия, оказывают profound влияние на экосистемы по всей планете, вызывая беспокойство у научного сообщества и требуя незамедлительных мер.\",\n",
    "        \n",
    "        \"Экономическая глобализация, с одной стороны, способствует развитию международной торговли и культурному обмену, а с другой — создает определенные challenges для национальных экономик и социальных структур.\",\n",
    "        \n",
    "        \"Квантовые вычисления, основанные на принципах квантовой механики, promise революционизировать области криптографии, молекулярного моделирования и оптимизации сложных систем в ближайшие годы.\",\n",
    "        \n",
    "        \"Философские концепции, разработанные античными мыслителями, продолжают оказывать значительное влияние на современную этику, политическую теорию и методологию научного познания.\",\n",
    "        \n",
    "        \"Биотехнологический прогресс последних лет позволяет редактировать геномы живых организмов с unprecedented точностью, открывая новые возможности в медицине и сельском хозяйстве.\",\n",
    "        \n",
    "        \"Архитектурные памятники древних цивилизаций, сохранившиеся до наших дней, представляют собой не только историческую ценность, но и источник вдохновения для современных зодчих.\",\n",
    "        \n",
    "        \"Психолингвистические исследования демонстрируют, как языковые структуры влияют на процессы восприятия, категоризации и запоминания информации у носителей разных языков.\"\n",
    "    ]\n",
    "\n",
    "    domain_texts = [\n",
    "        # Technical/scientific texts with complex terminology\n",
    "        \"Трансформерные архитектуры, основанные на механизмах внимания, кардинально изменили подходы к обработке естественного языка, демонстрируя state-of-the-art результаты в задачах машинного перевода и генерации текста.\",\n",
    "        \n",
    "        \"Глубокое обучение с подкреплением, сочетающее в себе нейронные сети и методы динамического программирования, успешно применяется для решения сложных задач управления в robotics и игровых симуляторах.\",\n",
    "        \n",
    "        \"Диффузионные probabilistic модели, генерирующие данные через процесс последовательной денойзинга, показали remarkable результаты в синтезе изображений высокого разрешения и аудиосигналов.\",\n",
    "        \n",
    "        \"Метаобучение или learning-to-learn подходы позволяют моделям быстро адаптироваться к новым задачам на основе limited количества примеров, имитируя способность человека к быстрому обучению.\",\n",
    "        \n",
    "        \"Мультимодальные нейронные сети, способные одновременно обрабатывать и интегрировать информацию из различных источников such as текст, изображения и аудио, открывают новые перспективы для создания более интеллектуальных AI систем.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Creating main audio files with Google TTS...\")\n",
    "    \n",
    "    # Create main audio files\n",
    "    for i, text in enumerate(main_texts, 1):\n",
    "        # Create audio using Google TTS\n",
    "        tts = gTTS(text=text, lang='ru', slow=False)\n",
    "        audio_file = f\"raw_audio/main_audio_{i}.wav\"\n",
    "        tts.save(audio_file)\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        # Save reference text\n",
    "        with open(f\"reference_texts/audio{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\" Created: reference_texts/audio{i}.txt\")\n",
    "    \n",
    "    print(\"\\nCreating domain audio files...\")\n",
    "    \n",
    "    # Create domain audio files\n",
    "    for i, text in enumerate(domain_texts, 1):\n",
    "        # Create audio using Google TTS\n",
    "        tts = gTTS(text=text, lang='ru', slow=False)\n",
    "        audio_file = f\"raw_audio/domain_audio_{i}.wav\"\n",
    "        tts.save(audio_file)\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        # Save reference text\n",
    "        with open(f\"reference_texts/domain{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Created: reference_texts/domain{i}.txt\")\n",
    "    \n",
    "    print(f\"\\n Created {len(main_texts)} main audio files and {len(domain_texts)} domain audio files!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_audio_files()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade2a1b",
   "metadata": {},
   "source": [
    "### Use Existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d986ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Common Voice Russian dataset...\n",
      "Error loading dataset: Dataset 'mozilla-foundation/common_voice_16_1' is a gated dataset on the Hub. You must be authenticated to access it.\n"
     ]
    }
   ],
   "source": [
    "# create_audio_tts.py\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "def create_audio_files():\n",
    "    \"\"\"Create Russian audio files using Common Voice Russian dataset\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"raw_audio\", exist_ok=True)\n",
    "    os.makedirs(\"reference_texts\", exist_ok=True)\n",
    "    \n",
    "    print(\"Loading Common Voice Russian dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Use Common Voice Russian dataset\n",
    "        dataset = load_dataset(\"mozilla-foundation/common_voice_16_1\", \"ru\", split=\"train\", streaming=True)\n",
    "        print(\"Common Voice Russian dataset loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Find Russian samples with audio and text\n",
    "    russian_samples = []\n",
    "    for sample in dataset:\n",
    "        if len(russian_samples) >= 15:\n",
    "            break\n",
    "            \n",
    "        if has_valid_audio(sample) and has_russian_text(sample):\n",
    "            russian_samples.append(sample)\n",
    "            print(f\"Found Russian sample {len(russian_samples)}\")\n",
    "    \n",
    "    print(f\"Found {len(russian_samples)} suitable Russian samples\")\n",
    "    \n",
    "    if len(russian_samples) < 15:\n",
    "        print(f\"Error: Need 15 Russian samples but only found {len(russian_samples)}\")\n",
    "        return\n",
    "    \n",
    "    # Select 15 samples (10 for main, 5 for domain)\n",
    "    selected_samples = russian_samples[:15]\n",
    "    \n",
    "    print(\"Creating main audio files...\")\n",
    "    \n",
    "    # Create main audio files (10 files)\n",
    "    for i in range(10):\n",
    "        sample = selected_samples[i]\n",
    "        text = get_text(sample)\n",
    "        audio_data = sample['audio']\n",
    "        \n",
    "        audio_file = f\"raw_audio/main_audio_{i+1}.wav\"\n",
    "        sf.write(audio_file, audio_data['array'], audio_data['sampling_rate'])\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        with open(f\"reference_texts/audio{i+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Created: reference_texts/audio{i+1}.txt\")\n",
    "    \n",
    "    print(\"\\nCreating domain audio files...\")\n",
    "    \n",
    "    # Create domain audio files (5 files)\n",
    "    for i in range(5):\n",
    "        sample = selected_samples[i + 10]\n",
    "        text = get_text(sample)\n",
    "        audio_data = sample['audio']\n",
    "        \n",
    "        audio_file = f\"raw_audio/domain_audio_{i+1}.wav\"\n",
    "        sf.write(audio_file, audio_data['array'], audio_data['sampling_rate'])\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        with open(f\"reference_texts/domain{i+1}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Created: reference_texts/domain{i+1}.txt\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully created 10 main audio files and 5 domain audio files!\")\n",
    "\n",
    "def has_valid_audio(sample):\n",
    "    \"\"\"Check if sample has valid audio data\"\"\"\n",
    "    if 'audio' not in sample or sample['audio'] is None:\n",
    "        return False\n",
    "    \n",
    "    audio_data = sample['audio']\n",
    "    if 'array' not in audio_data or audio_data['array'] is None:\n",
    "        return False\n",
    "    \n",
    "    if len(audio_data['array']) == 0:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def has_russian_text(sample):\n",
    "    \"\"\"Check if sample has Russian text\"\"\"\n",
    "    text = get_text(sample)\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Check for Cyrillic characters\n",
    "    return any('\\u0400' <= char <= '\\u04FF' for char in text)\n",
    "\n",
    "def get_text(sample):\n",
    "    \"\"\"Extract text from sample\"\"\"\n",
    "    if 'sentence' in sample and sample['sentence']:\n",
    "        return sample['sentence'].strip()\n",
    "    elif 'text' in sample and sample['text']:\n",
    "        return sample['text'].strip()\n",
    "    elif 'transcription' in sample and sample['transcription']:\n",
    "        return sample['transcription'].strip()\n",
    "    return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_audio_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadab27f",
   "metadata": {},
   "source": [
    "### Convert all raw audio to 16kHz mono WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8a7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio to 16kHz mono WAV...\n"
     ]
    }
   ],
   "source": [
    "def convert_audio_files():\n",
    "    print(\"Converting audio to 16kHz mono WAV...\")\n",
    "    \n",
    "    # Convert main audio files (10 files)\n",
    "    for i in range(1, 11):\n",
    "        input_file = f\"raw_audio/main_audio_{i}.wav\"\n",
    "        output_file = f\"audio_files/audio{i}.wav\"\n",
    "        \n",
    "        if os.path.exists(input_file):\n",
    "            audio, sr = librosa.load(input_file, sr=16000, mono=True)\n",
    "            sf.write(output_file, audio, 16000, subtype='PCM_16')\n",
    "            print(f\"✓ Converted: {output_file}\")\n",
    "    \n",
    "    # Convert domain audio files (5 files)  \n",
    "    for i in range(1, 6):\n",
    "        input_file = f\"raw_audio/domain_audio_{i}.wav\"\n",
    "        output_file = f\"domain_audio/domain{i}.wav\"\n",
    "        \n",
    "        if os.path.exists(input_file):\n",
    "            audio, sr = librosa.load(input_file, sr=16000, mono=True)\n",
    "            sf.write(output_file, audio, 16000, subtype='PCM_16')\n",
    "            print(f\"✓ Converted: {output_file}\")\n",
    "\n",
    "convert_audio_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1c67f",
   "metadata": {},
   "source": [
    "### Organizing reference texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe640d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing reference texts...\n",
      "✓ ALL FILES ORGANIZED!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"Organizing reference texts...\")\n",
    "\n",
    "# Copy main reference texts\n",
    "for i in range(1, 11):\n",
    "    src = f\"reference_texts/audio{i}.txt\"\n",
    "    dst = f\"audio_files/audio{i}.txt\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"✓ Copied: {dst}\")\n",
    "\n",
    "# Copy domain reference texts  \n",
    "for i in range(1, 6):\n",
    "    src = f\"reference_texts/domain{i}.txt\"\n",
    "    dst = f\"domain_audio/domain{i}.txt\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"✓ Copied: {dst}\")\n",
    "\n",
    "print(\"✓ ALL FILES ORGANIZED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ASRComparator:\n",
    "    def __init__(self):\n",
    "        self.whisper_model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")\n",
    "        self.vosk_model = self._init_vosk()\n",
    "        self.giga_model = self._init_gigaam()\n",
    "    \n",
    "    def _init_vosk(self):\n",
    "        \"\"\"Initialize Vosk model with comprehensive error handling\"\"\"\n",
    "        model_path = \"vosk-model-small-ru-0.22\"\n",
    "        \n",
    "        # Check if model directory exists\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Vosk model directory not found: {model_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if model directory has necessary files\n",
    "        required_folders = ['am', 'conf', 'graph']\n",
    "        if not all(os.path.exists(os.path.join(model_path, folder)) for folder in required_folders):\n",
    "            print(f\"Vosk model directory is incomplete: {model_path}\")\n",
    "            print(\"   Please re-download the model\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            model = Model(model_path)\n",
    "            print(\"Vosk model loaded successfully!\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Vosk model: {e}\")\n",
    "            print(\"   Model path might be incorrect or model files are corrupted\")\n",
    "            return None\n",
    "    \n",
    "    def _init_gigaam(self):\n",
    "        \"\"\"Initialize GigaAM with the correct method you found\"\"\"\n",
    "        try:\n",
    "            import gigaam\n",
    "            print(\"Loading GigaAM model...\")\n",
    "            model = gigaam.load_model(\"ctc\")\n",
    "            print(\"GigaAM model loaded successfully!\")\n",
    "            return model\n",
    "        except ImportError as e:\n",
    "            print(f\"GigaAM import failed: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load GigaAM model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_wer_cer(self, reference, hypothesis):\n",
    "        \"\"\"Calculate WER and CER using jiwer\"\"\"\n",
    "        # Clean the texts\n",
    "        reference = reference.strip()\n",
    "        hypothesis = hypothesis.strip()\n",
    "        \n",
    "        if not reference or not hypothesis:\n",
    "            return 1.0, 1.0  # Return maximum error if empty\n",
    "        \n",
    "        # WER (Word Error Rate)\n",
    "        wer = jiwer.wer(reference, hypothesis)\n",
    "        \n",
    "        # CER (Character Error Rate)\n",
    "        cer = jiwer.cer(reference, hypothesis)\n",
    "        \n",
    "        return wer, cer\n",
    "    \n",
    "    def whisper_transcribe(self, audio_path, prompt=None):\n",
    "        \"\"\"Transcribe using Whisper with optional prompt\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            segments, info = self.whisper_model.transcribe(\n",
    "                audio_path,\n",
    "                language=\"ru\",\n",
    "                initial_prompt=prompt,\n",
    "                beam_size=5\n",
    "            )\n",
    "            \n",
    "            transcription = \" \".join([segment.text for segment in segments])\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return transcription.strip(), end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"Whisper transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def vosk_transcribe(self, audio_path):\n",
    "        \"\"\"Transcribe using Vosk\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Read audio file\n",
    "            wf = wave.open(audio_path, \"rb\")\n",
    "            if wf.getnchannels() != 1:\n",
    "                print(f\"Warning: {audio_path} is not mono\")\n",
    "            if wf.getsampwidth() != 2:\n",
    "                print(f\"Warning: {audio_path} is not 16-bit\")\n",
    "            if wf.getframerate() != 16000:\n",
    "                print(f\"Warning: {audio_path} is not 16kHz\")\n",
    "            \n",
    "            rec = KaldiRecognizer(self.vosk_model, wf.getframerate())\n",
    "            rec.SetWords(True)\n",
    "            \n",
    "            transcription = \"\"\n",
    "            while True:\n",
    "                data = wf.readframes(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if rec.AcceptWaveform(data):\n",
    "                    result = json.loads(rec.Result())\n",
    "                    transcription += result.get(\"text\", \"\") + \" \"\n",
    "            \n",
    "            # Get final result\n",
    "            result = json.loads(rec.FinalResult())\n",
    "            transcription += result.get(\"text\", \"\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            wf.close()\n",
    "            \n",
    "            return transcription.strip(), end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"Vosk transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def giga_transcribe(self, audio_path):\n",
    "        \"\"\"Transcribe using GigaAM\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            transcription = self.giga_model.transcribe(audio_path)\n",
    "                      \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return transcription, end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"GigaAM transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def calculate_rtf(self, processing_time, audio_duration):\n",
    "        \"\"\"Calculate Real Time Factor\"\"\"\n",
    "        if audio_duration == 0:\n",
    "            return 0.0\n",
    "        return processing_time / audio_duration\n",
    "    \n",
    "    def get_audio_duration(self, audio_path):\n",
    "        \"\"\"Get audio duration in seconds\"\"\"\n",
    "        try:\n",
    "            with wave.open(audio_path, 'rb') as wf:\n",
    "                frames = wf.getnframes()\n",
    "                rate = wf.getframerate()\n",
    "                return frames / float(rate)\n",
    "        except:\n",
    "            # Fallback using librosa\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            return len(audio) / sr\n",
    "    \n",
    "    def run_comparison(self, audio_dir, reference_dir):\n",
    "        \"\"\"Run comparison on all audio files\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"No WAV files found in {audio_dir}\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} audio files for comparison\")\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(audio_dir, audio_file)\n",
    "            ref_path = os.path.join(reference_dir, audio_file.replace('.wav', '.txt'))\n",
    "            \n",
    "            # Check if reference file exists\n",
    "            if not os.path.exists(ref_path):\n",
    "                print(f\"Reference file {ref_path} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load reference text\n",
    "            with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "                reference_text = f.read().strip()\n",
    "            \n",
    "            audio_duration = self.get_audio_duration(audio_path)\n",
    "            print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "            print(f\"Reference text: {reference_text}\")\n",
    "            \n",
    "            # Whisper without prompt\n",
    "            print(\"Transcribing with Whisper...\")\n",
    "            whisper_text, whisper_time = self.whisper_transcribe(audio_path)\n",
    "            whisper_wer, whisper_cer = self.calculate_wer_cer(reference_text, whisper_text)\n",
    "            whisper_rtf = self.calculate_rtf(whisper_time, audio_duration)\n",
    "            print(f\"Whisper result: {whisper_text}\")\n",
    "            \n",
    "            # Vosk\n",
    "            print(\"Transcribing with Vosk...\")\n",
    "            vosk_text, vosk_time = self.vosk_transcribe(audio_path)\n",
    "            vosk_wer, vosk_cer = self.calculate_wer_cer(reference_text, vosk_text)\n",
    "            vosk_rtf = self.calculate_rtf(vosk_time, audio_duration)\n",
    "            print(f\"Vosk result: {vosk_text}\")\n",
    "            \n",
    "            # GigaAM\n",
    "            print(\"Transcribing with GigaAM...\")\n",
    "            giga_text, giga_time = self.giga_transcribe(audio_path)\n",
    "            giga_wer, giga_cer = self.calculate_wer_cer(reference_text, giga_text)\n",
    "            giga_rtf = self.calculate_rtf(giga_time, audio_duration)\n",
    "            print(f\"GigaAM result: {giga_text}\")\n",
    "            \n",
    "            results.append({\n",
    "                'file': audio_file,\n",
    "                'duration': audio_duration,\n",
    "                'reference': reference_text,\n",
    "                'whisper': {\n",
    "                    'text': whisper_text, \n",
    "                    'wer': whisper_wer, \n",
    "                    'cer': whisper_cer, \n",
    "                    'rtf': whisper_rtf,\n",
    "                    'time': whisper_time\n",
    "                },\n",
    "                'vosk': {\n",
    "                    'text': vosk_text,\n",
    "                    'wer': vosk_wer, \n",
    "                    'cer': vosk_cer, \n",
    "                    'rtf': vosk_rtf,\n",
    "                    'time': vosk_time\n",
    "                },\n",
    "                'giga': {\n",
    "                    'text': giga_text,\n",
    "                    'wer': giga_wer, \n",
    "                    'cer': giga_cer, \n",
    "                    'rtf': giga_rtf,\n",
    "                    'time': giga_time\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {audio_file}\")\n",
    "            print(f\"Whisper - WER: {whisper_wer:.3f}, CER: {whisper_cer:.3f}, RTF: {whisper_rtf:.3f}\")\n",
    "            print(f\"Vosk - WER: {vosk_wer:.3f}, CER: {vosk_cer:.3f}, RTF: {vosk_rtf:.3f}\")\n",
    "            print(f\"GigaAM - WER: {giga_wer:.3f}, CER: {giga_cer:.3f}, RTF: {giga_rtf:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_prompt_impact(self, domain_audio_dir, domain_terms):\n",
    "        \"\"\"Evaluate the impact of bias prompt in Whisper\"\"\"\n",
    "        prompt = \" \".join(domain_terms)\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        \n",
    "        audio_files = [f for f in os.listdir(domain_audio_dir) if f.endswith('.wav')]\n",
    "        results = []\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"No WAV files found in {domain_audio_dir}\")\n",
    "            return results\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(domain_audio_dir, audio_file)\n",
    "            ref_path = os.path.join(domain_audio_dir, audio_file.replace('.wav', '.txt'))\n",
    "            \n",
    "            if not os.path.exists(ref_path):\n",
    "                print(f\"Reference file {ref_path} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load reference text\n",
    "            with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "                reference_text = f.read().strip()\n",
    "            \n",
    "            print(f\"\\nDomain audio: {audio_file}\")\n",
    "            print(f\"Reference: {reference_text}\")\n",
    "            \n",
    "            # Without prompt\n",
    "            text_no_prompt, time_no_prompt = self.whisper_transcribe(audio_path)\n",
    "            wer_no_prompt, cer_no_prompt = self.calculate_wer_cer(reference_text, text_no_prompt)\n",
    "            print(f\"No prompt result: {text_no_prompt}\")\n",
    "            \n",
    "            # With prompt\n",
    "            text_with_prompt, time_with_prompt = self.whisper_transcribe(audio_path, prompt)\n",
    "            wer_with_prompt, cer_with_prompt = self.calculate_wer_cer(reference_text, text_with_prompt)\n",
    "            print(f\"With prompt result: {text_with_prompt}\")\n",
    "            \n",
    "            results.append({\n",
    "                'file': audio_file,\n",
    "                'reference': reference_text,\n",
    "                'no_prompt': {\n",
    "                    'text': text_no_prompt,\n",
    "                    'wer': wer_no_prompt, \n",
    "                    'cer': cer_no_prompt,\n",
    "                    'time': time_no_prompt\n",
    "                },\n",
    "                'with_prompt': {\n",
    "                    'text': text_with_prompt,\n",
    "                    'wer': wer_with_prompt, \n",
    "                    'cer': cer_with_prompt,\n",
    "                    'time': time_with_prompt\n",
    "                },\n",
    "                'improvement_wer': wer_no_prompt - wer_with_prompt,\n",
    "                'improvement_cer': cer_no_prompt - cer_with_prompt\n",
    "            })\n",
    "            \n",
    "            print(f\"No prompt - WER: {wer_no_prompt:.3f}, CER: {cer_no_prompt:.3f}\")\n",
    "            print(f\"With prompt - WER: {wer_with_prompt:.3f}, CER: {cer_with_prompt:.3f}\")\n",
    "            print(f\"Improvement - WER: {results[-1]['improvement_wer']:.3f}, CER: {results[-1]['improvement_cer']:.3f}\")\n",
    "            print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def create_summary_table(self, results):\n",
    "        \"\"\"Create a comprehensive summary table of results\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to summarize\")\n",
    "            return None\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            summary_data.append({\n",
    "                'File': result['file'],\n",
    "                'Duration': f\"{result['duration']:.2f}s\",\n",
    "                'Whisper_WER': f\"{result['whisper']['wer']:.3f}\",\n",
    "                'Whisper_CER': f\"{result['whisper']['cer']:.3f}\",\n",
    "                'Whisper_RTF': f\"{result['whisper']['rtf']:.3f}\",\n",
    "                'Vosk_WER': f\"{result['vosk']['wer']:.3f}\",\n",
    "                'Vosk_CER': f\"{result['vosk']['cer']:.3f}\",\n",
    "                'Vosk_RTF': f\"{result['vosk']['rtf']:.3f}\",\n",
    "                'GigaAM_WER': f\"{result['giga']['wer']:.3f}\",\n",
    "                'GigaAM_CER': f\"{result['giga']['cer']:.3f}\",\n",
    "                'GigaAM_RTF': f\"{result['giga']['rtf']:.3f}\",\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add averages row\n",
    "        avg_row = {\n",
    "            'File': 'AVERAGE',\n",
    "            'Duration': '-',\n",
    "            'Whisper_WER': f\"{np.mean([r['whisper']['wer'] for r in results]):.3f}\",\n",
    "            'Whisper_CER': f\"{np.mean([r['whisper']['cer'] for r in results]):.3f}\",\n",
    "            'Whisper_RTF': f\"{np.mean([r['whisper']['rtf'] for r in results]):.3f}\",\n",
    "            'Vosk_WER': f\"{np.mean([r['vosk']['wer'] for r in results]):.3f}\",\n",
    "            'Vosk_CER': f\"{np.mean([r['vosk']['cer'] for r in results]):.3f}\",\n",
    "            'Vosk_RTF': f\"{np.mean([r['vosk']['rtf'] for r in results]):.3f}\",\n",
    "            'GigaAM_WER': f\"{np.mean([r['giga']['wer'] for r in results]):.3f}\",\n",
    "            'GigaAM_CER': f\"{np.mean([r['giga']['cer'] for r in results]):.3f}\",\n",
    "            'GigaAM_RTF': f\"{np.mean([r['giga']['rtf'] for r in results]):.3f}\",\n",
    "        }\n",
    "        \n",
    "        df_avg = pd.DataFrame([avg_row])\n",
    "        df = pd.concat([df, df_avg], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_prompt_summary_table(self, prompt_results):\n",
    "        \"\"\"Create summary table for prompt impact results\"\"\"\n",
    "        if not prompt_results:\n",
    "            print(\"No prompt results to summarize\")\n",
    "            return None\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for result in prompt_results:\n",
    "            summary_data.append({\n",
    "                'File': result['file'],\n",
    "                'Reference': result['reference'][:50] + '...' if len(result['reference']) > 50 else result['reference'],\n",
    "                'NoPrompt_WER': f\"{result['no_prompt']['wer']:.3f}\",\n",
    "                'NoPrompt_CER': f\"{result['no_prompt']['cer']:.3f}\",\n",
    "                'WithPrompt_WER': f\"{result['with_prompt']['wer']:.3f}\",\n",
    "                'WithPrompt_CER': f\"{result['with_prompt']['cer']:.3f}\",\n",
    "                'WER_Improvement': f\"{result['improvement_wer']:.3f}\",\n",
    "                'CER_Improvement': f\"{result['improvement_cer']:.3f}\",\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add averages row\n",
    "        avg_row = {\n",
    "            'File': 'AVERAGE',\n",
    "            'Reference': '-',\n",
    "            'NoPrompt_WER': f\"{np.mean([r['no_prompt']['wer'] for r in prompt_results]):.3f}\",\n",
    "            'NoPrompt_CER': f\"{np.mean([r['no_prompt']['cer'] for r in prompt_results]):.3f}\",\n",
    "            'WithPrompt_WER': f\"{np.mean([r['with_prompt']['wer'] for r in prompt_results]):.3f}\",\n",
    "            'WithPrompt_CER': f\"{np.mean([r['with_prompt']['cer'] for r in prompt_results]):.3f}\",\n",
    "            'WER_Improvement': f\"{np.mean([r['improvement_wer'] for r in prompt_results]):.3f}\",\n",
    "            'CER_Improvement': f\"{np.mean([r['improvement_cer'] for r in prompt_results]):.3f}\",\n",
    "        }\n",
    "        \n",
    "        df_avg = pd.DataFrame([avg_row])\n",
    "        df = pd.concat([df, df_avg], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fc49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-ru-0.22/graph/HCLr.fst vosk-model-small-ru-0.22/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vosk model loaded successfully!\n",
      "Loading GigaAM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:fp16 is not supported on CPU. Leaving fp32 weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAM model loaded successfully!\n",
      "=== ASR Engines Comparison ===\n",
      "No WAV files found in audio_files\n",
      "\n",
      "\n",
      "=== Prompt Impact Evaluation ===\n",
      "Using prompt: специфический терминология доменная контекст профессиональный\n",
      "No WAV files found in domain_audio\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    comparator = ASRComparator()\n",
    "    \n",
    "    # Part 1: Compare ASR engines\n",
    "    print(\"=== ASR Engines Comparison ===\")\n",
    "    main_results = comparator.run_comparison(\"audio_files\", \"reference_texts\")\n",
    "    \n",
    "    # Create and display summary table\n",
    "    if main_results:\n",
    "        summary_df = comparator.create_summary_table(main_results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SUMMARY TABLE - ASR ENGINES COMPARISON\")\n",
    "        print(\"=\"*100)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Save to CSV\n",
    "        summary_df.to_csv(\"/results/asr_comparison_results.csv\", index=False)\n",
    "        print(\"\\nResults saved to 'asr_comparison_results.csv'\")\n",
    "    \n",
    "    # Part 2: Evaluate prompt impact\n",
    "    print(\"\\n\\n=== Prompt Impact Evaluation ===\")\n",
    "    domain_terms = [\"специфический\", \"терминология\", \"доменная\", \"контекст\", \"профессиональный\"]\n",
    "    prompt_results = comparator.evaluate_prompt_impact(\"domain_audio\", domain_terms)\n",
    "    \n",
    "    # Create and display prompt summary table\n",
    "    if prompt_results:\n",
    "        prompt_summary_df = comparator.create_prompt_summary_table(prompt_results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SUMMARY TABLE - PROMPT IMPACT EVALUATION\")\n",
    "        print(\"=\"*100)\n",
    "        print(prompt_summary_df.to_string(index=False))\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Save to CSV\n",
    "        prompt_summary_df.to_csv(\"/results/prompt_impact_results.csv\", index=False)\n",
    "        print(\"\\nPrompt results saved to 'prompt_impact_results.csv'\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
