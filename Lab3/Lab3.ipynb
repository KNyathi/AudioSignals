{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8038cc3",
   "metadata": {},
   "source": [
    "## Лабораторная Работа 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb13fb",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f31aa97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-8cndnq_3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-8cndnq_3\n",
      "  Resolved https://github.com/huggingface/transformers to commit 9aab965b1e61d92d402809bd467c317ec464e560\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (2025.9.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub==1.0.0.rc6 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (1.0.0rc6)\n",
      "Requirement already satisfied: typer-slim in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.20.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==5.0.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/nyathi/.local/lib/python3.10/site-packages (from transformers==5.0.0.dev0) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.1.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->transformers==5.0.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==5.0.0.dev0) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->transformers==5.0.0.dev0) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==5.0.0.dev0) (2020.6.20)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/nyathi/.local/lib/python3.10/site-packages (from typer-slim->transformers==5.0.0.dev0) (8.1.8)\n",
      "Requirement already satisfied: anyio in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/nyathi/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "077d1cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faster-whisper in /home/nyathi/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: vosk in /home/nyathi/.local/lib/python3.10/site-packages (0.3.45)\n",
      "Requirement already satisfied: jiwer in /home/nyathi/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: librosa in /home/nyathi/.local/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /home/nyathi/.local/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: pydub in /home/nyathi/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: torch in /home/nyathi/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchaudio in /home/nyathi/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: gtts in /home/nyathi/.local/lib/python3.10/site-packages (2.5.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (1.0.0rc6)\n",
      "Requirement already satisfied: av>=11 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (16.0.1)\n",
      "Requirement already satisfied: tokenizers<1,>=0.13 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (0.22.1)\n",
      "Requirement already satisfied: tqdm in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (4.67.1)\n",
      "Requirement already satisfied: onnxruntime<2,>=1.14 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (1.23.1)\n",
      "Requirement already satisfied: ctranslate2<5,>=4.0 in /home/nyathi/.local/lib/python3.10/site-packages (from faster-whisper) (4.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (2.0.0)\n",
      "Requirement already satisfied: requests in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (2.32.5)\n",
      "Requirement already satisfied: websockets in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (15.0.1)\n",
      "Requirement already satisfied: srt in /home/nyathi/.local/lib/python3.10/site-packages (from vosk) (3.5.3)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/nyathi/.local/lib/python3.10/site-packages (from jiwer) (3.14.1)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/nyathi/.local/lib/python3.10/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /home/nyathi/.local/lib/python3.10/site-packages (from librosa) (1.24.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: fsspec in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/nyathi/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/nyathi/.local/lib/python3.10/site-packages (from cffi>=1.0->vosk) (2.23)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.9.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/lib/python3/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (5.4.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (0.28.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.10)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (25.0)\n",
      "Requirement already satisfied: typer-slim in /home/nyathi/.local/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (0.20.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /home/nyathi/.local/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
      "Requirement already satisfied: flatbuffers in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
      "Requirement already satisfied: coloredlogs in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
      "Requirement already satisfied: protobuf in /home/nyathi/.local/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.25.8)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/nyathi/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->vosk) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->vosk) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nyathi/.local/lib/python3.10/site-packages (from requests->vosk) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->vosk) (2020.6.20)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nyathi/.local/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyathi/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: anyio in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster-whisper) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nyathi/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster-whisper) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/nyathi/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.13->faster-whisper) (0.16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/nyathi/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.13->faster-whisper) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/nyathi/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.13->faster-whisper) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faster-whisper vosk jiwer librosa soundfile pydub torch torchaudio gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4db29977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faster_whisper import WhisperModel\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import jiwer\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7296",
   "metadata": {},
   "source": [
    "### Download Vosk and Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53732d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete\n",
      "Extraction complete\n",
      "Cleanup complete\n",
      "Model contents: ['README', 'ivector', 'am', 'graph', 'conf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-ru-0.22/graph/HCLr.fst vosk-model-small-ru-0.22/graph/Gr.fst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOSK MODEL IS WORKING!\n",
      "Model path: /home/nyathi/AudioSignals/Lab3/vosk-model-small-ru-0.22\n",
      "\n",
      "READY! You can now run your main ASR comparison.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n"
     ]
    }
   ],
   "source": [
    "def download_and_setup_vosk():\n",
    "    model_url = \"https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip\"\n",
    "    model_zip = \"vosk-model-small-ru-0.22.zip\"\n",
    "    model_dir = \"vosk-model-small-ru-0.22\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Download the model\n",
    "        urllib.request.urlretrieve(model_url, model_zip)\n",
    "        print(\"Download complete\")\n",
    "        \n",
    "        # Extract the model\n",
    "        with zipfile.ZipFile(model_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        print(\"Extraction complete\")\n",
    "        \n",
    "        # Clean up zip file\n",
    "        os.remove(model_zip)\n",
    "        print(\"Cleanup complete\")\n",
    "        \n",
    "        # Verify the model structure\n",
    "        if os.path.exists(model_dir):\n",
    "            contents = os.listdir(model_dir)\n",
    "            print(f\"Model contents: {contents}\")\n",
    "            \n",
    "            # Test if model works\n",
    "            from vosk import Model\n",
    "            try:\n",
    "                test_model = Model(model_dir)\n",
    "                print(\"VOSK MODEL IS WORKING!\")\n",
    "                print(f\"Model path: {os.path.abspath(model_dir)}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Model test failed: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Model directory not created\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = download_and_setup_vosk()\n",
    "    if success:\n",
    "        print(\"\\nREADY! You can now run your main ASR comparison.\")\n",
    "    else:\n",
    "        print(\"\\nFailed to set up Vosk model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbd5d7",
   "metadata": {},
   "source": [
    "### Checking GigaAM installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9205c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyathi/GigaAM/gigaam/__init__.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "WARNING:root:fp16 is not supported on CPU. Leaving fp32 weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAMASR(\n",
      "  (preprocessor): FeatureExtractor(\n",
      "    (featurizer): Sequential(\n",
      "      (0): MelSpectrogram(\n",
      "        (spectrogram): Spectrogram()\n",
      "        (mel_scale): MelScale()\n",
      "      )\n",
      "      (1): SpecScaler()\n",
      "    )\n",
      "  )\n",
      "  (encoder): ConformerEncoder(\n",
      "    (pre_encode): StridingSubsampling(\n",
      "      (out): Linear(in_features=12288, out_features=768, bias=True)\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pos_enc): RotaryPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x ConformerLayer(\n",
      "        (norm_feed_forward1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward1): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_conv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): ConformerConvolution(\n",
      "          (pointwise_conv1): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(768, 768, kernel_size=(31,), stride=(1,), padding=(15,), groups=768)\n",
      "          (batch_norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): SiLU()\n",
      "          (pointwise_conv2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (norm_self_att): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): RotaryPositionMultiHeadAttention(\n",
      "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_feed_forward2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward2): ConformerFeedForward(\n",
      "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm_out): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): CTCHead(\n",
      "    (decoder_layers): Sequential(\n",
      "      (0): Conv1d(768, 34, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import gigaam\n",
    "model = gigaam.load_model(\"ctc\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7766aff",
   "metadata": {},
   "source": [
    "### Downloading audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c82d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating main audio files with Google TTS...\n",
      "Created: raw_audio/main_audio_1.wav\n",
      " Created: reference_texts/audio1.txt\n",
      "Created: raw_audio/main_audio_2.wav\n",
      " Created: reference_texts/audio2.txt\n",
      "Created: raw_audio/main_audio_3.wav\n",
      " Created: reference_texts/audio3.txt\n",
      "Created: raw_audio/main_audio_4.wav\n",
      " Created: reference_texts/audio4.txt\n",
      "Created: raw_audio/main_audio_5.wav\n",
      " Created: reference_texts/audio5.txt\n",
      "Created: raw_audio/main_audio_6.wav\n",
      " Created: reference_texts/audio6.txt\n",
      "Created: raw_audio/main_audio_7.wav\n",
      " Created: reference_texts/audio7.txt\n",
      "Created: raw_audio/main_audio_8.wav\n",
      " Created: reference_texts/audio8.txt\n",
      "Created: raw_audio/main_audio_9.wav\n",
      " Created: reference_texts/audio9.txt\n",
      "Created: raw_audio/main_audio_10.wav\n",
      " Created: reference_texts/audio10.txt\n",
      "\n",
      "Creating domain audio files...\n",
      "Created: raw_audio/domain_audio_1.wav\n",
      "Created: reference_texts/domain1.txt\n",
      "Created: raw_audio/domain_audio_2.wav\n",
      "Created: reference_texts/domain2.txt\n",
      "Created: raw_audio/domain_audio_3.wav\n",
      "Created: reference_texts/domain3.txt\n",
      "Created: raw_audio/domain_audio_4.wav\n",
      "Created: reference_texts/domain4.txt\n",
      "Created: raw_audio/domain_audio_5.wav\n",
      "Created: reference_texts/domain5.txt\n",
      "\n",
      " Created 10 main audio files and 5 domain audio files!\n"
     ]
    }
   ],
   "source": [
    "# create_audio_tts.py\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "def create_audio_files():\n",
    "    \"\"\"Create Russian audio files using Google TTS\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"raw_audio\", exist_ok=True)\n",
    "    os.makedirs(\"reference_texts\", exist_ok=True)\n",
    "    \n",
    "    # Russian texts for main comparison (10 files)\n",
    "    main_texts = [\n",
    "        # Longer, more complex sentences with varied vocabulary\n",
    "        \"В современном мире искусственный интеллект стремительно развивается и находит применение в различных сферах человеческой деятельности, начиная от медицины и заканчивая финансовыми технологиями.\",\n",
    "        \n",
    "        \"Несмотря на значительные достижения в области машинного обучения, создание по-настоящему разумных систем, способных к абстрактному мышлению и творчеству, остается сложнейшей задачей для исследователей.\",\n",
    "        \n",
    "        \"Русский язык, являясь одним из самых богатых и выразительных языков мира, обладает сложной грамматической структурой и многообразием стилистических средств, что представляет особый интерес для лингвистического анализа.\",\n",
    "        \n",
    "        \"Климатические изменения, наблюдаемые в последние десятилетия, оказывают profound влияние на экосистемы по всей планете, вызывая беспокойство у научного сообщества и требуя незамедлительных мер.\",\n",
    "        \n",
    "        \"Экономическая глобализация, с одной стороны, способствует развитию международной торговли и культурному обмену, а с другой — создает определенные challenges для национальных экономик и социальных структур.\",\n",
    "        \n",
    "        \"Квантовые вычисления, основанные на принципах квантовой механики, promise революционизировать области криптографии, молекулярного моделирования и оптимизации сложных систем в ближайшие годы.\",\n",
    "        \n",
    "        \"Философские концепции, разработанные античными мыслителями, продолжают оказывать значительное влияние на современную этику, политическую теорию и методологию научного познания.\",\n",
    "        \n",
    "        \"Биотехнологический прогресс последних лет позволяет редактировать геномы живых организмов с unprecedented точностью, открывая новые возможности в медицине и сельском хозяйстве.\",\n",
    "        \n",
    "        \"Архитектурные памятники древних цивилизаций, сохранившиеся до наших дней, представляют собой не только историческую ценность, но и источник вдохновения для современных зодчих.\",\n",
    "        \n",
    "        \"Психолингвистические исследования демонстрируют, как языковые структуры влияют на процессы восприятия, категоризации и запоминания информации у носителей разных языков.\"\n",
    "    ]\n",
    "\n",
    "    domain_texts = [\n",
    "        # Technical/scientific texts with complex terminology\n",
    "        \"Трансформерные архитектуры, основанные на механизмах внимания, кардинально изменили подходы к обработке естественного языка, демонстрируя state-of-the-art результаты в задачах машинного перевода и генерации текста.\",\n",
    "        \n",
    "        \"Глубокое обучение с подкреплением, сочетающее в себе нейронные сети и методы динамического программирования, успешно применяется для решения сложных задач управления в robotics и игровых симуляторах.\",\n",
    "        \n",
    "        \"Диффузионные probabilistic модели, генерирующие данные через процесс последовательной денойзинга, показали remarkable результаты в синтезе изображений высокого разрешения и аудиосигналов.\",\n",
    "        \n",
    "        \"Метаобучение или learning-to-learn подходы позволяют моделям быстро адаптироваться к новым задачам на основе limited количества примеров, имитируя способность человека к быстрому обучению.\",\n",
    "        \n",
    "        \"Мультимодальные нейронные сети, способные одновременно обрабатывать и интегрировать информацию из различных источников such as текст, изображения и аудио, открывают новые перспективы для создания более интеллектуальных AI систем.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Creating main audio files with Google TTS...\")\n",
    "    \n",
    "    # Create main audio files\n",
    "    for i, text in enumerate(main_texts, 1):\n",
    "        # Create audio using Google TTS\n",
    "        tts = gTTS(text=text, lang='ru', slow=False)\n",
    "        audio_file = f\"raw_audio/main_audio_{i}.wav\"\n",
    "        tts.save(audio_file)\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        # Save reference text\n",
    "        with open(f\"reference_texts/audio{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\" Created: reference_texts/audio{i}.txt\")\n",
    "    \n",
    "    print(\"\\nCreating domain audio files...\")\n",
    "    \n",
    "    # Create domain audio files\n",
    "    for i, text in enumerate(domain_texts, 1):\n",
    "        # Create audio using Google TTS\n",
    "        tts = gTTS(text=text, lang='ru', slow=False)\n",
    "        audio_file = f\"raw_audio/domain_audio_{i}.wav\"\n",
    "        tts.save(audio_file)\n",
    "        print(f\"Created: {audio_file}\")\n",
    "        \n",
    "        # Save reference text\n",
    "        with open(f\"reference_texts/domain{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Created: reference_texts/domain{i}.txt\")\n",
    "    \n",
    "    print(f\"\\n Created {len(main_texts)} main audio files and {len(domain_texts)} domain audio files!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_audio_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadab27f",
   "metadata": {},
   "source": [
    "### Convert all raw audio to 16kHz mono WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b8a7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio to 16kHz mono WAV...\n",
      "✓ Converted: audio_files/audio1.wav\n",
      "✓ Converted: audio_files/audio2.wav\n",
      "✓ Converted: audio_files/audio3.wav\n",
      "✓ Converted: audio_files/audio4.wav\n",
      "✓ Converted: audio_files/audio5.wav\n",
      "✓ Converted: audio_files/audio6.wav\n",
      "✓ Converted: audio_files/audio7.wav\n",
      "✓ Converted: audio_files/audio8.wav\n",
      "✓ Converted: audio_files/audio9.wav\n",
      "✓ Converted: audio_files/audio10.wav\n",
      "✓ Converted: domain_audio/domain1.wav\n",
      "✓ Converted: domain_audio/domain2.wav\n",
      "✓ Converted: domain_audio/domain3.wav\n",
      "✓ Converted: domain_audio/domain4.wav\n",
      "✓ Converted: domain_audio/domain5.wav\n"
     ]
    }
   ],
   "source": [
    "def convert_audio_files():\n",
    "    print(\"Converting audio to 16kHz mono WAV...\")\n",
    "    \n",
    "    # Convert main audio files (10 files)\n",
    "    for i in range(1, 11):\n",
    "        input_file = f\"raw_audio/main_audio_{i}.wav\"\n",
    "        output_file = f\"audio_files/audio{i}.wav\"\n",
    "        \n",
    "        if os.path.exists(input_file):\n",
    "            audio, sr = librosa.load(input_file, sr=16000, mono=True)\n",
    "            sf.write(output_file, audio, 16000, subtype='PCM_16')\n",
    "            print(f\"✓ Converted: {output_file}\")\n",
    "    \n",
    "    # Convert domain audio files (5 files)  \n",
    "    for i in range(1, 6):\n",
    "        input_file = f\"raw_audio/domain_audio_{i}.wav\"\n",
    "        output_file = f\"domain_audio/domain{i}.wav\"\n",
    "        \n",
    "        if os.path.exists(input_file):\n",
    "            audio, sr = librosa.load(input_file, sr=16000, mono=True)\n",
    "            sf.write(output_file, audio, 16000, subtype='PCM_16')\n",
    "            print(f\"✓ Converted: {output_file}\")\n",
    "\n",
    "convert_audio_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1c67f",
   "metadata": {},
   "source": [
    "### Organizing reference texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfe640d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing reference texts...\n",
      "✓ Copied: audio_files/audio1.txt\n",
      "✓ Copied: audio_files/audio2.txt\n",
      "✓ Copied: audio_files/audio3.txt\n",
      "✓ Copied: audio_files/audio4.txt\n",
      "✓ Copied: audio_files/audio5.txt\n",
      "✓ Copied: audio_files/audio6.txt\n",
      "✓ Copied: audio_files/audio7.txt\n",
      "✓ Copied: audio_files/audio8.txt\n",
      "✓ Copied: audio_files/audio9.txt\n",
      "✓ Copied: audio_files/audio10.txt\n",
      "✓ Copied: domain_audio/domain1.txt\n",
      "✓ Copied: domain_audio/domain2.txt\n",
      "✓ Copied: domain_audio/domain3.txt\n",
      "✓ Copied: domain_audio/domain4.txt\n",
      "✓ Copied: domain_audio/domain5.txt\n",
      "✓ ALL FILES ORGANIZED!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"Organizing reference texts...\")\n",
    "\n",
    "# Copy main reference texts\n",
    "for i in range(1, 11):\n",
    "    src = f\"reference_texts/audio{i}.txt\"\n",
    "    dst = f\"audio_files/audio{i}.txt\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"✓ Copied: {dst}\")\n",
    "\n",
    "# Copy domain reference texts  \n",
    "for i in range(1, 6):\n",
    "    src = f\"reference_texts/domain{i}.txt\"\n",
    "    dst = f\"domain_audio/domain{i}.txt\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"✓ Copied: {dst}\")\n",
    "\n",
    "print(\"✓ ALL FILES ORGANIZED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ASRComparator:\n",
    "    def __init__(self):\n",
    "        self.whisper_model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")\n",
    "        self.vosk_model = self._init_vosk()\n",
    "        self.giga_model = self._init_gigaam()\n",
    "    \n",
    "    def _init_vosk(self):\n",
    "        \"\"\"Initialize Vosk model with comprehensive error handling\"\"\"\n",
    "        model_path = \"vosk-model-small-ru-0.22\"\n",
    "        \n",
    "        # Check if model directory exists\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Vosk model directory not found: {model_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Check if model directory has necessary files\n",
    "        required_folders = ['am', 'conf', 'graph']\n",
    "        if not all(os.path.exists(os.path.join(model_path, folder)) for folder in required_folders):\n",
    "            print(f\"Vosk model directory is incomplete: {model_path}\")\n",
    "            print(\"   Please re-download the model\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            model = Model(model_path)\n",
    "            print(\"Vosk model loaded successfully!\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load Vosk model: {e}\")\n",
    "            print(\"   Model path might be incorrect or model files are corrupted\")\n",
    "            return None\n",
    "    \n",
    "    def _init_gigaam(self):\n",
    "        \"\"\"Initialize GigaAM with the correct method you found\"\"\"\n",
    "        try:\n",
    "            import gigaam\n",
    "            print(\"Loading GigaAM model...\")\n",
    "            model = gigaam.load_model(\"ctc\")\n",
    "            print(\"GigaAM model loaded successfully!\")\n",
    "            return model\n",
    "        except ImportError as e:\n",
    "            print(f\"GigaAM import failed: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load GigaAM model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_wer_cer(self, reference, hypothesis):\n",
    "        \"\"\"Calculate WER and CER using jiwer\"\"\"\n",
    "        # Clean the texts\n",
    "        reference = reference.strip()\n",
    "        hypothesis = hypothesis.strip()\n",
    "        \n",
    "        if not reference or not hypothesis:\n",
    "            return 1.0, 1.0  # Return maximum error if empty\n",
    "        \n",
    "        # WER (Word Error Rate)\n",
    "        wer = jiwer.wer(reference, hypothesis)\n",
    "        \n",
    "        # CER (Character Error Rate)\n",
    "        cer = jiwer.cer(reference, hypothesis)\n",
    "        \n",
    "        return wer, cer\n",
    "    \n",
    "    def whisper_transcribe(self, audio_path, prompt=None):\n",
    "        \"\"\"Transcribe using Whisper with optional prompt\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            segments, info = self.whisper_model.transcribe(\n",
    "                audio_path,\n",
    "                language=\"ru\",\n",
    "                initial_prompt=prompt,\n",
    "                beam_size=5\n",
    "            )\n",
    "            \n",
    "            transcription = \" \".join([segment.text for segment in segments])\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return transcription.strip(), end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"Whisper transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def vosk_transcribe(self, audio_path):\n",
    "        \"\"\"Transcribe using Vosk\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Read audio file\n",
    "            wf = wave.open(audio_path, \"rb\")\n",
    "            if wf.getnchannels() != 1:\n",
    "                print(f\"Warning: {audio_path} is not mono\")\n",
    "            if wf.getsampwidth() != 2:\n",
    "                print(f\"Warning: {audio_path} is not 16-bit\")\n",
    "            if wf.getframerate() != 16000:\n",
    "                print(f\"Warning: {audio_path} is not 16kHz\")\n",
    "            \n",
    "            rec = KaldiRecognizer(self.vosk_model, wf.getframerate())\n",
    "            rec.SetWords(True)\n",
    "            \n",
    "            transcription = \"\"\n",
    "            while True:\n",
    "                data = wf.readframes(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if rec.AcceptWaveform(data):\n",
    "                    result = json.loads(rec.Result())\n",
    "                    transcription += result.get(\"text\", \"\") + \" \"\n",
    "            \n",
    "            # Get final result\n",
    "            result = json.loads(rec.FinalResult())\n",
    "            transcription += result.get(\"text\", \"\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            wf.close()\n",
    "            \n",
    "            return transcription.strip(), end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"Vosk transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def giga_transcribe(self, audio_path):\n",
    "        \"\"\"Transcribe using GigaAM\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            transcription = self.giga_model.transcribe(audio_path)\n",
    "                      \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return transcription, end_time - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"GigaAM transcription error: {e}\")\n",
    "            return \"\", 0.0\n",
    "    \n",
    "    def calculate_rtf(self, processing_time, audio_duration):\n",
    "        \"\"\"Calculate Real Time Factor\"\"\"\n",
    "        if audio_duration == 0:\n",
    "            return 0.0\n",
    "        return processing_time / audio_duration\n",
    "    \n",
    "    def get_audio_duration(self, audio_path):\n",
    "        \"\"\"Get audio duration in seconds\"\"\"\n",
    "        try:\n",
    "            with wave.open(audio_path, 'rb') as wf:\n",
    "                frames = wf.getnframes()\n",
    "                rate = wf.getframerate()\n",
    "                return frames / float(rate)\n",
    "        except:\n",
    "            # Fallback using librosa\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            return len(audio) / sr\n",
    "    \n",
    "    def run_comparison(self, audio_dir, reference_dir):\n",
    "        \"\"\"Run comparison on all audio files\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"No WAV files found in {audio_dir}\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} audio files for comparison\")\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(audio_dir, audio_file)\n",
    "            ref_path = os.path.join(reference_dir, audio_file.replace('.wav', '.txt'))\n",
    "            \n",
    "            # Check if reference file exists\n",
    "            if not os.path.exists(ref_path):\n",
    "                print(f\"Reference file {ref_path} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load reference text\n",
    "            with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "                reference_text = f.read().strip()\n",
    "            \n",
    "            audio_duration = self.get_audio_duration(audio_path)\n",
    "            print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "            print(f\"Reference text: {reference_text}\")\n",
    "            \n",
    "            # Whisper without prompt\n",
    "            print(\"Transcribing with Whisper...\")\n",
    "            whisper_text, whisper_time = self.whisper_transcribe(audio_path)\n",
    "            whisper_wer, whisper_cer = self.calculate_wer_cer(reference_text, whisper_text)\n",
    "            whisper_rtf = self.calculate_rtf(whisper_time, audio_duration)\n",
    "            print(f\"Whisper result: {whisper_text}\")\n",
    "            \n",
    "            # Vosk\n",
    "            print(\"Transcribing with Vosk...\")\n",
    "            vosk_text, vosk_time = self.vosk_transcribe(audio_path)\n",
    "            vosk_wer, vosk_cer = self.calculate_wer_cer(reference_text, vosk_text)\n",
    "            vosk_rtf = self.calculate_rtf(vosk_time, audio_duration)\n",
    "            print(f\"Vosk result: {vosk_text}\")\n",
    "            \n",
    "            # GigaAM\n",
    "            print(\"Transcribing with GigaAM...\")\n",
    "            giga_text, giga_time = self.giga_transcribe(audio_path)\n",
    "            giga_wer, giga_cer = self.calculate_wer_cer(reference_text, giga_text)\n",
    "            giga_rtf = self.calculate_rtf(giga_time, audio_duration)\n",
    "            print(f\"GigaAM result: {giga_text}\")\n",
    "            \n",
    "            results.append({\n",
    "                'file': audio_file,\n",
    "                'duration': audio_duration,\n",
    "                'reference': reference_text,\n",
    "                'whisper': {\n",
    "                    'text': whisper_text, \n",
    "                    'wer': whisper_wer, \n",
    "                    'cer': whisper_cer, \n",
    "                    'rtf': whisper_rtf,\n",
    "                    'time': whisper_time\n",
    "                },\n",
    "                'vosk': {\n",
    "                    'text': vosk_text,\n",
    "                    'wer': vosk_wer, \n",
    "                    'cer': vosk_cer, \n",
    "                    'rtf': vosk_rtf,\n",
    "                    'time': vosk_time\n",
    "                },\n",
    "                'giga': {\n",
    "                    'text': giga_text,\n",
    "                    'wer': giga_wer, \n",
    "                    'cer': giga_cer, \n",
    "                    'rtf': giga_rtf,\n",
    "                    'time': giga_time\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {audio_file}\")\n",
    "            print(f\"Whisper - WER: {whisper_wer:.3f}, CER: {whisper_cer:.3f}, RTF: {whisper_rtf:.3f}\")\n",
    "            print(f\"Vosk - WER: {vosk_wer:.3f}, CER: {vosk_cer:.3f}, RTF: {vosk_rtf:.3f}\")\n",
    "            print(f\"GigaAM - WER: {giga_wer:.3f}, CER: {giga_cer:.3f}, RTF: {giga_rtf:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_prompt_impact(self, domain_audio_dir, domain_terms):\n",
    "        \"\"\"Evaluate the impact of bias prompt in Whisper\"\"\"\n",
    "        prompt = \" \".join(domain_terms)\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        \n",
    "        audio_files = [f for f in os.listdir(domain_audio_dir) if f.endswith('.wav')]\n",
    "        results = []\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"No WAV files found in {domain_audio_dir}\")\n",
    "            return results\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(domain_audio_dir, audio_file)\n",
    "            ref_path = os.path.join(domain_audio_dir, audio_file.replace('.wav', '.txt'))\n",
    "            \n",
    "            if not os.path.exists(ref_path):\n",
    "                print(f\"Reference file {ref_path} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load reference text\n",
    "            with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "                reference_text = f.read().strip()\n",
    "            \n",
    "            print(f\"\\nDomain audio: {audio_file}\")\n",
    "            print(f\"Reference: {reference_text}\")\n",
    "            \n",
    "            # Without prompt\n",
    "            text_no_prompt, time_no_prompt = self.whisper_transcribe(audio_path)\n",
    "            wer_no_prompt, cer_no_prompt = self.calculate_wer_cer(reference_text, text_no_prompt)\n",
    "            print(f\"No prompt result: {text_no_prompt}\")\n",
    "            \n",
    "            # With prompt\n",
    "            text_with_prompt, time_with_prompt = self.whisper_transcribe(audio_path, prompt)\n",
    "            wer_with_prompt, cer_with_prompt = self.calculate_wer_cer(reference_text, text_with_prompt)\n",
    "            print(f\"With prompt result: {text_with_prompt}\")\n",
    "            \n",
    "            results.append({\n",
    "                'file': audio_file,\n",
    "                'reference': reference_text,\n",
    "                'no_prompt': {\n",
    "                    'text': text_no_prompt,\n",
    "                    'wer': wer_no_prompt, \n",
    "                    'cer': cer_no_prompt,\n",
    "                    'time': time_no_prompt\n",
    "                },\n",
    "                'with_prompt': {\n",
    "                    'text': text_with_prompt,\n",
    "                    'wer': wer_with_prompt, \n",
    "                    'cer': cer_with_prompt,\n",
    "                    'time': time_with_prompt\n",
    "                },\n",
    "                'improvement_wer': wer_no_prompt - wer_with_prompt,\n",
    "                'improvement_cer': cer_no_prompt - cer_with_prompt\n",
    "            })\n",
    "            \n",
    "            print(f\"No prompt - WER: {wer_no_prompt:.3f}, CER: {cer_no_prompt:.3f}\")\n",
    "            print(f\"With prompt - WER: {wer_with_prompt:.3f}, CER: {cer_with_prompt:.3f}\")\n",
    "            print(f\"Improvement - WER: {results[-1]['improvement_wer']:.3f}, CER: {results[-1]['improvement_cer']:.3f}\")\n",
    "            print(\"=\" * 80)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def create_summary_table(self, results):\n",
    "        \"\"\"Create a comprehensive summary table of results\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to summarize\")\n",
    "            return None\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            summary_data.append({\n",
    "                'File': result['file'],\n",
    "                'Duration': f\"{result['duration']:.2f}s\",\n",
    "                'Whisper_WER': f\"{result['whisper']['wer']:.3f}\",\n",
    "                'Whisper_CER': f\"{result['whisper']['cer']:.3f}\",\n",
    "                'Whisper_RTF': f\"{result['whisper']['rtf']:.3f}\",\n",
    "                'Vosk_WER': f\"{result['vosk']['wer']:.3f}\",\n",
    "                'Vosk_CER': f\"{result['vosk']['cer']:.3f}\",\n",
    "                'Vosk_RTF': f\"{result['vosk']['rtf']:.3f}\",\n",
    "                'GigaAM_WER': f\"{result['giga']['wer']:.3f}\",\n",
    "                'GigaAM_CER': f\"{result['giga']['cer']:.3f}\",\n",
    "                'GigaAM_RTF': f\"{result['giga']['rtf']:.3f}\",\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add averages row\n",
    "        avg_row = {\n",
    "            'File': 'AVERAGE',\n",
    "            'Duration': '-',\n",
    "            'Whisper_WER': f\"{np.mean([r['whisper']['wer'] for r in results]):.3f}\",\n",
    "            'Whisper_CER': f\"{np.mean([r['whisper']['cer'] for r in results]):.3f}\",\n",
    "            'Whisper_RTF': f\"{np.mean([r['whisper']['rtf'] for r in results]):.3f}\",\n",
    "            'Vosk_WER': f\"{np.mean([r['vosk']['wer'] for r in results]):.3f}\",\n",
    "            'Vosk_CER': f\"{np.mean([r['vosk']['cer'] for r in results]):.3f}\",\n",
    "            'Vosk_RTF': f\"{np.mean([r['vosk']['rtf'] for r in results]):.3f}\",\n",
    "            'GigaAM_WER': f\"{np.mean([r['giga']['wer'] for r in results]):.3f}\",\n",
    "            'GigaAM_CER': f\"{np.mean([r['giga']['cer'] for r in results]):.3f}\",\n",
    "            'GigaAM_RTF': f\"{np.mean([r['giga']['rtf'] for r in results]):.3f}\",\n",
    "        }\n",
    "        \n",
    "        df_avg = pd.DataFrame([avg_row])\n",
    "        df = pd.concat([df, df_avg], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_prompt_summary_table(self, prompt_results):\n",
    "        \"\"\"Create summary table for prompt impact results\"\"\"\n",
    "        if not prompt_results:\n",
    "            print(\"No prompt results to summarize\")\n",
    "            return None\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for result in prompt_results:\n",
    "            summary_data.append({\n",
    "                'File': result['file'],\n",
    "                'Reference': result['reference'][:50] + '...' if len(result['reference']) > 50 else result['reference'],\n",
    "                'NoPrompt_WER': f\"{result['no_prompt']['wer']:.3f}\",\n",
    "                'NoPrompt_CER': f\"{result['no_prompt']['cer']:.3f}\",\n",
    "                'WithPrompt_WER': f\"{result['with_prompt']['wer']:.3f}\",\n",
    "                'WithPrompt_CER': f\"{result['with_prompt']['cer']:.3f}\",\n",
    "                'WER_Improvement': f\"{result['improvement_wer']:.3f}\",\n",
    "                'CER_Improvement': f\"{result['improvement_cer']:.3f}\",\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add averages row\n",
    "        avg_row = {\n",
    "            'File': 'AVERAGE',\n",
    "            'Reference': '-',\n",
    "            'NoPrompt_WER': f\"{np.mean([r['no_prompt']['wer'] for r in prompt_results]):.3f}\",\n",
    "            'NoPrompt_CER': f\"{np.mean([r['no_prompt']['cer'] for r in prompt_results]):.3f}\",\n",
    "            'WithPrompt_WER': f\"{np.mean([r['with_prompt']['wer'] for r in prompt_results]):.3f}\",\n",
    "            'WithPrompt_CER': f\"{np.mean([r['with_prompt']['cer'] for r in prompt_results]):.3f}\",\n",
    "            'WER_Improvement': f\"{np.mean([r['improvement_wer'] for r in prompt_results]):.3f}\",\n",
    "            'CER_Improvement': f\"{np.mean([r['improvement_cer'] for r in prompt_results]):.3f}\",\n",
    "        }\n",
    "        \n",
    "        df_avg = pd.DataFrame([avg_row])\n",
    "        df = pd.concat([df, df_avg], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22fc49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-ru-0.22/graph/HCLr.fst vosk-model-small-ru-0.22/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n",
      "/home/nyathi/GigaAM/gigaam/__init__.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vosk model loaded successfully!\n",
      "Loading GigaAM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:fp16 is not supported on CPU. Leaving fp32 weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAM model loaded successfully!\n",
      "=== ASR Engines Comparison ===\n",
      "Found 10 audio files for comparison\n",
      "Audio duration: 3.29 seconds\n",
      "Reference text: лингвистика изучает структуры языка\n",
      "Transcribing with Whisper...\n",
      "Whisper result: лингвистика изучает структуры языка.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: лингвистика изучает структуры языка\n",
      "Transcribing with GigaAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyathi/GigaAM/gigaam/preprocess.py:40: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1560.)\n",
      "  return torch.frombuffer(audio, dtype=torch.int16).float() / 32768.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAM result: лингвистика изучает структуры языка\n",
      "Processed audio9.wav\n",
      "Whisper - WER: 0.250, CER: 0.029, RTF: 0.660\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.370\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.300\n",
      "--------------------------------------------------\n",
      "Audio duration: 4.06 seconds\n",
      "Reference text: глубокие нейронные сети сложны но эффективны\n",
      "Transcribing with Whisper...\n",
      "Whisper result: глубокие неронные сети сложны, но эффективны.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: глубокие нейронные сети сложны но и эффективны\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: глубокие нейронные сети сложны но эффективны\n",
      "Processed audio8.wav\n",
      "Whisper - WER: 0.500, CER: 0.068, RTF: 0.527\n",
      "Vosk - WER: 0.167, CER: 0.045, RTF: 0.321\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.269\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.72 seconds\n",
      "Reference text: распознавание речи становится всё точнее\n",
      "Transcribing with Whisper...\n",
      "Whisper result: распознавание речи становится все точнее.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: распознавания речи становятся все точнее\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: распознавание речи становится все точнее\n",
      "Processed audio4.wav\n",
      "Whisper - WER: 0.400, CER: 0.050, RTF: 0.555\n",
      "Vosk - WER: 0.600, CER: 0.075, RTF: 0.371\n",
      "GigaAM - WER: 0.200, CER: 0.025, RTF: 0.273\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.22 seconds\n",
      "Reference text: русский язык богат и выразителен\n",
      "Transcribing with Whisper...\n",
      "Whisper result: русский язык богат и выразителен.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: русский язык богат и выразителен\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: русский язык богат и выразителен\n",
      "Processed audio5.wav\n",
      "Whisper - WER: 0.200, CER: 0.031, RTF: 0.626\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.383\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.285\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.24 seconds\n",
      "Reference text: машинное обучение используется везде\n",
      "Transcribing with Whisper...\n",
      "Whisper result: Машинное обучение используется везде.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: машинное обучение используется везде\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: машинное обучение используется везде\n",
      "Processed audio7.wav\n",
      "Whisper - WER: 0.500, CER: 0.056, RTF: 0.627\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.377\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.285\n",
      "--------------------------------------------------\n",
      "Audio duration: 2.18 seconds\n",
      "Reference text: привет как дела сегодня\n",
      "Transcribing with Whisper...\n",
      "Whisper result: Привет, как дела сегодня?\n",
      "Transcribing with Vosk...\n",
      "Vosk result: привет как дела сегодня\n",
      "Transcribing with GigaAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1020 23:52:31.236918891 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaAM result: привет как дела сегодня\n",
      "Processed audio1.wav\n",
      "Whisper - WER: 0.500, CER: 0.130, RTF: 0.887\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.472\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.322\n",
      "--------------------------------------------------\n",
      "Audio duration: 2.98 seconds\n",
      "Reference text: фонетика анализирует звуки речи\n",
      "Transcribing with Whisper...\n",
      "Whisper result: Фанэтик анализирует звуки речи.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: фонетика анализирует звуки речи\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: фонетика анализирует звуки речи\n",
      "Processed audio10.wav\n",
      "Whisper - WER: 0.500, CER: 0.161, RTF: 0.707\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.389\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.297\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.62 seconds\n",
      "Reference text: научные исследования важны для прогресса\n",
      "Transcribing with Whisper...\n",
      "Whisper result: Научные исследования важны для прогресса.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: научные исследования важны для прогресса\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: научные исследования важны для прогресса\n",
      "Processed audio6.wav\n",
      "Whisper - WER: 0.400, CER: 0.050, RTF: 0.553\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.352\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.275\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.55 seconds\n",
      "Reference text: погода в москве очень красивая осенью\n",
      "Transcribing with Whisper...\n",
      "Whisper result: Погода в Москве очень красивая осенью.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: погода в москве очень красивая осенью\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: погода в москве очень красивая осенью\n",
      "Processed audio2.wav\n",
      "Whisper - WER: 0.500, CER: 0.081, RTF: 0.578\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.373\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.275\n",
      "--------------------------------------------------\n",
      "Audio duration: 3.72 seconds\n",
      "Reference text: искусственный интеллект преобразует технологии\n",
      "Transcribing with Whisper...\n",
      "Whisper result: искусственный интеллект преобразует технологии.\n",
      "Transcribing with Vosk...\n",
      "Vosk result: искусственный интеллект преобразует технологии\n",
      "Transcribing with GigaAM...\n",
      "GigaAM result: искусственный интеллект преобразует технологии\n",
      "Processed audio3.wav\n",
      "Whisper - WER: 0.250, CER: 0.022, RTF: 0.548\n",
      "Vosk - WER: 0.000, CER: 0.000, RTF: 0.377\n",
      "GigaAM - WER: 0.000, CER: 0.000, RTF: 0.272\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY TABLE - ASR ENGINES COMPARISON\n",
      "====================================================================================================\n",
      "       File Duration Whisper_WER Whisper_CER Whisper_RTF Vosk_WER Vosk_CER Vosk_RTF GigaAM_WER GigaAM_CER GigaAM_RTF\n",
      " audio9.wav    3.29s       0.250       0.029       0.660    0.000    0.000    0.370      0.000      0.000      0.300\n",
      " audio8.wav    4.06s       0.500       0.068       0.527    0.167    0.045    0.321      0.000      0.000      0.269\n",
      " audio4.wav    3.72s       0.400       0.050       0.555    0.600    0.075    0.371      0.200      0.025      0.273\n",
      " audio5.wav    3.22s       0.200       0.031       0.626    0.000    0.000    0.383      0.000      0.000      0.285\n",
      " audio7.wav    3.24s       0.500       0.056       0.627    0.000    0.000    0.377      0.000      0.000      0.285\n",
      " audio1.wav    2.18s       0.500       0.130       0.887    0.000    0.000    0.472      0.000      0.000      0.322\n",
      "audio10.wav    2.98s       0.500       0.161       0.707    0.000    0.000    0.389      0.000      0.000      0.297\n",
      " audio6.wav    3.62s       0.400       0.050       0.553    0.000    0.000    0.352      0.000      0.000      0.275\n",
      " audio2.wav    3.55s       0.500       0.081       0.578    0.000    0.000    0.373      0.000      0.000      0.275\n",
      " audio3.wav    3.72s       0.250       0.022       0.548    0.000    0.000    0.377      0.000      0.000      0.272\n",
      "    AVERAGE        -       0.400       0.068       0.627    0.077    0.012    0.379      0.020      0.003      0.285\n",
      "====================================================================================================\n",
      "\n",
      "Results saved to 'asr_comparison_results.csv'\n",
      "\n",
      "\n",
      "=== Prompt Impact Evaluation ===\n",
      "Using prompt: специфический терминология доменная контекст профессиональный\n",
      "\n",
      "Domain audio: domain1.wav\n",
      "Reference: нейронные сети требуют больших данных для обучения\n",
      "No prompt result: Неронные сети требуют больших данных для обучения.\n",
      "With prompt result: нейронные сети требуют больших данных для обучения.\n",
      "No prompt - WER: 0.286, CER: 0.060\n",
      "With prompt - WER: 0.143, CER: 0.020\n",
      "Improvement - WER: 0.143, CER: 0.040\n",
      "================================================================================\n",
      "\n",
      "Domain audio: domain5.wav\n",
      "Reference: рекуррентные сети работают с последовательностями данных\n",
      "No prompt result: Рекурентные сети работают с последовательностями данных.\n",
      "With prompt result: Рекурентные сети работают с последовательностями данных.\n",
      "No prompt - WER: 0.333, CER: 0.054\n",
      "With prompt - WER: 0.333, CER: 0.054\n",
      "Improvement - WER: 0.000, CER: 0.000\n",
      "================================================================================\n",
      "\n",
      "Domain audio: domain4.wav\n",
      "Reference: свёрточные сети эффективны для компьютерного зрения\n",
      "No prompt result: сверточные сети эффективны для компьютерного зрения.\n",
      "With prompt result: сверточные сети эффективны для компьютерного зрения.\n",
      "No prompt - WER: 0.333, CER: 0.039\n",
      "With prompt - WER: 0.333, CER: 0.039\n",
      "Improvement - WER: 0.000, CER: 0.000\n",
      "================================================================================\n",
      "\n",
      "Domain audio: domain3.wav\n",
      "Reference: градиентный спуск оптимизирует параметры модели\n",
      "No prompt result: Градиентный спуск оптимизирует параметры модели.\n",
      "With prompt result: Градиентный спуск оптимизирует параметры модели.\n",
      "No prompt - WER: 0.400, CER: 0.043\n",
      "With prompt - WER: 0.400, CER: 0.043\n",
      "Improvement - WER: 0.000, CER: 0.000\n",
      "================================================================================\n",
      "\n",
      "Domain audio: domain2.wav\n",
      "Reference: трансформеры изменили обработку естественного языка\n",
      "No prompt result: Трансформеры изменили обработку естественного языка.\n",
      "With prompt result: трансформеры изменили обработку естественного языка.\n",
      "No prompt - WER: 0.400, CER: 0.039\n",
      "With prompt - WER: 0.200, CER: 0.020\n",
      "Improvement - WER: 0.200, CER: 0.020\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY TABLE - PROMPT IMPACT EVALUATION\n",
      "====================================================================================================\n",
      "       File                                             Reference NoPrompt_WER NoPrompt_CER WithPrompt_WER WithPrompt_CER WER_Improvement CER_Improvement\n",
      "domain1.wav    нейронные сети требуют больших данных для обучения        0.286        0.060          0.143          0.020           0.143           0.040\n",
      "domain5.wav рекуррентные сети работают с последовательностями ...        0.333        0.054          0.333          0.054           0.000           0.000\n",
      "domain4.wav свёрточные сети эффективны для компьютерного зрени...        0.333        0.039          0.333          0.039           0.000           0.000\n",
      "domain3.wav       градиентный спуск оптимизирует параметры модели        0.400        0.043          0.400          0.043           0.000           0.000\n",
      "domain2.wav трансформеры изменили обработку естественного язык...        0.400        0.039          0.200          0.020           0.200           0.020\n",
      "    AVERAGE                                                     -        0.350        0.047          0.282          0.035           0.069           0.012\n",
      "====================================================================================================\n",
      "\n",
      "Prompt results saved to 'prompt_impact_results.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    comparator = ASRComparator()\n",
    "    \n",
    "    # Part 1: Compare ASR engines\n",
    "    print(\"=== ASR Engines Comparison ===\")\n",
    "    main_results = comparator.run_comparison(\"audio_files\", \"reference_texts\")\n",
    "    \n",
    "    # Create and display summary table\n",
    "    if main_results:\n",
    "        summary_df = comparator.create_summary_table(main_results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SUMMARY TABLE - ASR ENGINES COMPARISON\")\n",
    "        print(\"=\"*100)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Save to CSV\n",
    "        summary_df.to_csv(\"asr_comparison_results.csv\", index=False)\n",
    "        print(\"\\nResults saved to 'asr_comparison_results.csv'\")\n",
    "    \n",
    "    # Part 2: Evaluate prompt impact\n",
    "    print(\"\\n\\n=== Prompt Impact Evaluation ===\")\n",
    "    domain_terms = [\"специфический\", \"терминология\", \"доменная\", \"контекст\", \"профессиональный\"]\n",
    "    prompt_results = comparator.evaluate_prompt_impact(\"domain_audio\", domain_terms)\n",
    "    \n",
    "    # Create and display prompt summary table\n",
    "    if prompt_results:\n",
    "        prompt_summary_df = comparator.create_prompt_summary_table(prompt_results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SUMMARY TABLE - PROMPT IMPACT EVALUATION\")\n",
    "        print(\"=\"*100)\n",
    "        print(prompt_summary_df.to_string(index=False))\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Save to CSV\n",
    "        prompt_summary_df.to_csv(\"prompt_impact_results.csv\", index=False)\n",
    "        print(\"\\nPrompt results saved to 'prompt_impact_results.csv'\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
